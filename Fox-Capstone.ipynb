{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "# Udacity Machine Learning Engineer Nanodegree\n",
    "## Capstone Project: \n",
    "## Predicting the Daily Direction of the S&P500\n",
    "\n",
    "### Completed and Submitted by Stephen Fox\n",
    "### October 2016\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Market data read successfully!\n",
      "('Number of data points:', 5962)\n"
     ]
    }
   ],
   "source": [
    "# Import libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from time import time\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "import re\n",
    "from IPython.display import display\n",
    "\n",
    "# Read data from SP500_historical.csv - data contains returns for the S&P500 and VIX index going back over 23 years\n",
    "# Source: Yahoo Finance\n",
    "\n",
    "all_data = pd.read_csv(\"SP500_historical.csv\",header=0)\n",
    "print (\"Market data read successfully!\")\n",
    "print (\"Number of data points:\", len(all_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Date', u'SP_Open', u'SP_High', u'SP_Low', u'SP_Close', u'SP_Volume',\n",
      "       u'Vix_Open', u'Vix_High', u'Vix_Low', u'Vix_Close'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the column headers\n",
    "print (all_data.dtypes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Convert the 'Date' column data from a string into a form that can be manipulated mathematically\n",
    "# Calculate the 'Days_Since_Open' feature, which is a measure of how many days have passed since the market last opened\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "all_data['Date'] = pd.to_datetime(all_data['Date'])\n",
    "\n",
    "all_data['Days_Since_Open'] = (all_data['Date'] - all_data['Date'].shift(-1))\n",
    "all_data['Days_Since_Open'] = all_data['Days_Since_Open'].astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11b127990>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEPCAYAAABRHfM8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11aed5c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize some of the key data\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plot_data = all_data.SP_Close\n",
    "plot_data_VIX = all_data.Vix_Close\n",
    "\n",
    "\n",
    "f, axes = plt.subplots(2, 1)\n",
    "\n",
    "axes[0].axis([5962,0,400,2200])\n",
    "axes[0].plot(plot_data)\n",
    "axes[0].set_ylabel('S&P500 Close')\n",
    "axes[0].set_yticks([400,800,1200,1600,2000,2400])\n",
    "\n",
    "axes[1].axis([5962,0,0,100])\n",
    "axes[1].plot(plot_data_VIX)\n",
    "axes[1].set_ylabel('VIX Close')\n",
    "plt.xlabel('Trading Days Since Most Recent Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.text.Text at 0x11b20b2d0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAZMAAAEPCAYAAACHuClZAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nwzCMkjFlYhiGYZSMKRPDMAyjZP4fTAKY6KpyrkoAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11a789390>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data_Volume = all_data.SP_Volume / 1000000\n",
    "\n",
    "plt.axis([5962,0,14,12000])\n",
    "plt.plot(plot_data_Volume)\n",
    "plt.ylabel('S&P500 Volume (in millions)')\n",
    "plt.xlabel('Trading Days Since Most Recent Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Break_Coming' feature, which is a measure of whether the market will close for a break on the next day\n",
    "\n",
    "all_data['Break_Coming'] = all_data['Days_Since_Open'].shift(1)\n",
    "\n",
    "all_data.loc[all_data['Break_Coming']<=1,'Break_Coming'] = 0\n",
    "all_data.loc[all_data['Break_Coming']>1,'Break_Coming'] = 1\n",
    "\n",
    "all_data.Break_Coming.fillna(0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Overnight_Return' feature\n",
    "\n",
    "all_data['Overnight_Return'] = all_data['SP_Open'] / all_data['SP_Close'].shift(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'No_Overnight_Change' value. This is not a feature, but was used to identify unreliable data rows\n",
    "# See accompanying report for further details\n",
    "\n",
    "all_data['No_Overnight_Change'] = all_data['Overnight_Return']\n",
    "\n",
    "all_data.loc[all_data['No_Overnight_Change']==0.00,'No_Overnight_Change'] = 1\n",
    "all_data.loc[all_data['No_Overnight_Change']!=1,'No_Overnight_Change'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Overnight_VIX' feature, which measures whether the VIX has opened up or down relative to previous day\n",
    "\n",
    "all_data['Overnight_VIX'] = all_data['Vix_Open'] / all_data['Vix_Close'].shift(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the 'O_to_O' feature, which compares the current opening price to the previous day\n",
    "\n",
    "all_data['O_to_O'] = all_data['SP_Open'] / all_data['SP_Open'].shift(-1) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate 11 different trailing return features, 9 based on the S&P500 data and 2 based on VIX data\n",
    "\n",
    "all_data['Trail_1d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-2) - 1\n",
    "all_data['Trail_2d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-3) - 1\n",
    "all_data['Trail_3d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-4) - 1\n",
    "all_data['Trail_4d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-5) - 1\n",
    "all_data['Trail_5d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-6) - 1\n",
    "all_data['Trail_21d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-22) - 1\n",
    "all_data['Trail_63d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-64) - 1\n",
    "all_data['Trail_126d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-127) - 1\n",
    "all_data['Trail_252d_Ret'] = all_data['SP_Close'].shift(-1) / all_data['SP_Close'].shift(-253) - 1\n",
    "\n",
    "all_data['Trail_1d_VIX'] = all_data['Vix_Close'].shift(-1) / all_data['Vix_Close'].shift(-2) - 1\n",
    "all_data['Trail_5d_VIX'] = all_data['Vix_Close'].shift(-1) / all_data['Vix_Close'].shift(-6) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate 2 different trailing volume features, which measure whether the last day's volume is above or below\n",
    "# recent averages\n",
    "\n",
    "all_data['Trail_1d_Rel_Vol'] = all_data['SP_Volume'].shift(-1) / (\n",
    "    all_data['SP_Volume'].rolling(window=4).mean().shift(-5)) - 1\n",
    "\n",
    "all_data['Trail_5d_Rel_Vol'] = all_data['SP_Volume'].rolling(window=5).mean().shift(-5) / (\n",
    "    all_data['SP_Volume'].rolling(window=21).mean().shift(-26)) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Trail_1d_PtT' feature, which measure how much the S&P500 moved from high to low values on the \n",
    "# previous trading day\n",
    "\n",
    "all_data['Trail_1d_PtT'] = (all_data['SP_High'].shift(-1) - all_data['SP_Low'].shift(-1)) / (0.5*(\n",
    "        all_data['SP_Open'].shift(-1) + all_data['SP_Close'].shift(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Trail_1d_VIX_PtT' feature, which measure how much the VIX moved from high to low values on the \n",
    "# previous trading day\n",
    "\n",
    "all_data['Trail_1d_VIX_PtT'] = (all_data['Vix_High'].shift(-1) - all_data['Vix_Low'].shift(-1)) / (0.5*(\n",
    "        all_data['Vix_Open'].shift(-1) + all_data['Vix_Close'].shift(-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Calculate the 'Intraday_Increase' label, which is ultimately what the machine learner should predict, and as is a\n",
    "# measure of whether the S&P500 increased on a given day (value = 1) or not (value = 0)\n",
    "\n",
    "all_data['Intraday_Increase'] = all_data['SP_Close'] - all_data['SP_Open']\n",
    "\n",
    "all_data.loc[all_data['Intraday_Increase']>0,'Intraday_Increase'] = 1\n",
    "all_data.loc[all_data['Intraday_Increase']<=0,'Intraday_Increase'] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I am now going to delete those rows that have N/A values (e.g. you cannot do a 252 day trailing return if there are fewer than 252 days trailing a given data point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of data points:', 5709)\n"
     ]
    }
   ],
   "source": [
    "noNA_data = all_data.dropna()\n",
    "\n",
    "print (\"Number of data points:\", len(noNA_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will also delete rows where 'No_Overnight_Change' is equal to 1. These are points where the opening value is IDENTICAL to the previous closing data, which likely reflects missing true opening prices (the chances of the market opening at the identical level as the previous close are slim to none). Most of these points came from the older portion of the data set. There were almost no points like this in the most recent 10 years:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Number of data points:', 2678)\n"
     ]
    }
   ],
   "source": [
    "validated_data = noNA_data[noNA_data.No_Overnight_Change != 1]\n",
    "print (\"Number of data points:\", len(validated_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, I need to drop columns that would allow the machine learning algorithm to 'know the future'. For example, it cannot be trained using SP_Close for any given day, since that would trivially allow it to calculate whether the market closed up on any given day. It can only be provided information that one would know at the start of the day (i.e. data from previous days and the opening data). This rules out High, Low and Close values for the market and the VIX and also the market volume for any given day. Finally, I will limit the analysis to relative returns, not absolute values of the indeces, given that the stock market tends to increase with time and hence comparing the absolute value of the S&P500 over a 20+ year period will not be helpful. \n",
    "\n",
    "In summary, the final data set will consist of only the 20 calculated features and the 1 label, for the 2,678 validated data rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Date', u'SP_Open', u'SP_High', u'SP_Low', u'SP_Close', u'SP_Volume',\n",
      "       u'Vix_Open', u'Vix_High', u'Vix_Low', u'Vix_Close', u'Days_Since_Open',\n",
      "       u'Break_Coming', u'Overnight_Return', u'No_Overnight_Change',\n",
      "       u'Overnight_VIX', u'O_to_O', u'Trail_1d_Ret', u'Trail_2d_Ret',\n",
      "       u'Trail_3d_Ret', u'Trail_4d_Ret', u'Trail_5d_Ret', u'Trail_21d_Ret',\n",
      "       u'Trail_63d_Ret', u'Trail_126d_Ret', u'Trail_252d_Ret', u'Trail_1d_VIX',\n",
      "       u'Trail_5d_VIX', u'Trail_1d_Rel_Vol', u'Trail_5d_Rel_Vol',\n",
      "       u'Trail_1d_PtT', u'Trail_1d_VIX_PtT', u'Intraday_Increase'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the column headers of the current data set\n",
    "print (validated_data.dtypes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Drop the columns that are not one of the 20 features or the label\n",
    "data = validated_data.drop(validated_data.columns[[0,1,2,3,4,5,6,7,8,9,13]], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index([u'Days_Since_Open', u'Break_Coming', u'Overnight_Return',\n",
      "       u'Overnight_VIX', u'O_to_O', u'Trail_1d_Ret', u'Trail_2d_Ret',\n",
      "       u'Trail_3d_Ret', u'Trail_4d_Ret', u'Trail_5d_Ret', u'Trail_21d_Ret',\n",
      "       u'Trail_63d_Ret', u'Trail_126d_Ret', u'Trail_252d_Ret', u'Trail_1d_VIX',\n",
      "       u'Trail_5d_VIX', u'Trail_1d_Rel_Vol', u'Trail_5d_Rel_Vol',\n",
      "       u'Trail_1d_PtT', u'Trail_1d_VIX_PtT', u'Intraday_Increase'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Print the headers of 'data', to validate that the correct columns were dropped\n",
    "print (data.dtypes.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: export the final data set that will be used for training / testing the algorithms\n",
    "np.savetxt(\"eval-data.csv\", data, delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Days_Since_Open</th>\n",
       "      <th>Break_Coming</th>\n",
       "      <th>Overnight_Return</th>\n",
       "      <th>Overnight_VIX</th>\n",
       "      <th>O_to_O</th>\n",
       "      <th>Trail_1d_Ret</th>\n",
       "      <th>Trail_2d_Ret</th>\n",
       "      <th>Trail_3d_Ret</th>\n",
       "      <th>Trail_4d_Ret</th>\n",
       "      <th>Trail_5d_Ret</th>\n",
       "      <th>...</th>\n",
       "      <th>Trail_63d_Ret</th>\n",
       "      <th>Trail_126d_Ret</th>\n",
       "      <th>Trail_252d_Ret</th>\n",
       "      <th>Trail_1d_VIX</th>\n",
       "      <th>Trail_5d_VIX</th>\n",
       "      <th>Trail_1d_Rel_Vol</th>\n",
       "      <th>Trail_5d_Rel_Vol</th>\n",
       "      <th>Trail_1d_PtT</th>\n",
       "      <th>Trail_1d_VIX_PtT</th>\n",
       "      <th>Intraday_Increase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.444735</td>\n",
       "      <td>0.210978</td>\n",
       "      <td>-0.000022</td>\n",
       "      <td>0.005908</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000260</td>\n",
       "      <td>0.000511</td>\n",
       "      <td>0.000612</td>\n",
       "      <td>0.000847</td>\n",
       "      <td>0.001112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.013449</td>\n",
       "      <td>0.029453</td>\n",
       "      <td>0.059645</td>\n",
       "      <td>0.002947</td>\n",
       "      <td>0.010982</td>\n",
       "      <td>0.008819</td>\n",
       "      <td>0.010441</td>\n",
       "      <td>0.013533</td>\n",
       "      <td>0.085928</td>\n",
       "      <td>0.547050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.875799</td>\n",
       "      <td>0.408079</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.045081</td>\n",
       "      <td>0.012292</td>\n",
       "      <td>0.012973</td>\n",
       "      <td>0.017303</td>\n",
       "      <td>0.020321</td>\n",
       "      <td>0.023078</td>\n",
       "      <td>0.025472</td>\n",
       "      <td>...</td>\n",
       "      <td>0.081302</td>\n",
       "      <td>0.124501</td>\n",
       "      <td>0.173760</td>\n",
       "      <td>0.074680</td>\n",
       "      <td>0.155039</td>\n",
       "      <td>0.181773</td>\n",
       "      <td>0.156196</td>\n",
       "      <td>0.011147</td>\n",
       "      <td>0.050513</td>\n",
       "      <td>0.497874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014194</td>\n",
       "      <td>-0.300366</td>\n",
       "      <td>-0.087119</td>\n",
       "      <td>-0.090350</td>\n",
       "      <td>-0.124174</td>\n",
       "      <td>-0.139059</td>\n",
       "      <td>-0.172221</td>\n",
       "      <td>-0.183401</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.417706</td>\n",
       "      <td>-0.464658</td>\n",
       "      <td>-0.488228</td>\n",
       "      <td>-0.295726</td>\n",
       "      <td>-0.426630</td>\n",
       "      <td>-0.832518</td>\n",
       "      <td>-0.669690</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.014966</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000407</td>\n",
       "      <td>-0.016031</td>\n",
       "      <td>-0.004256</td>\n",
       "      <td>-0.004494</td>\n",
       "      <td>-0.006988</td>\n",
       "      <td>-0.007946</td>\n",
       "      <td>-0.009736</td>\n",
       "      <td>-0.010137</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.022581</td>\n",
       "      <td>-0.027543</td>\n",
       "      <td>-0.004349</td>\n",
       "      <td>-0.040230</td>\n",
       "      <td>-0.079295</td>\n",
       "      <td>-0.090512</td>\n",
       "      <td>-0.083042</td>\n",
       "      <td>0.006880</td>\n",
       "      <td>0.053312</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000820</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000569</td>\n",
       "      <td>0.001294</td>\n",
       "      <td>0.002182</td>\n",
       "      <td>0.002630</td>\n",
       "      <td>0.003111</td>\n",
       "      <td>...</td>\n",
       "      <td>0.026604</td>\n",
       "      <td>0.043190</td>\n",
       "      <td>0.089677</td>\n",
       "      <td>-0.005022</td>\n",
       "      <td>-0.009048</td>\n",
       "      <td>-0.005982</td>\n",
       "      <td>-0.000374</td>\n",
       "      <td>0.010471</td>\n",
       "      <td>0.074426</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000500</td>\n",
       "      <td>0.024785</td>\n",
       "      <td>0.005425</td>\n",
       "      <td>0.005642</td>\n",
       "      <td>0.008789</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.014399</td>\n",
       "      <td>...</td>\n",
       "      <td>0.061822</td>\n",
       "      <td>0.097326</td>\n",
       "      <td>0.163089</td>\n",
       "      <td>0.036402</td>\n",
       "      <td>0.072292</td>\n",
       "      <td>0.090148</td>\n",
       "      <td>0.089807</td>\n",
       "      <td>0.016249</td>\n",
       "      <td>0.103512</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.510725</td>\n",
       "      <td>0.106712</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.132064</td>\n",
       "      <td>0.139480</td>\n",
       "      <td>0.179735</td>\n",
       "      <td>0.191112</td>\n",
       "      <td>...</td>\n",
       "      <td>0.388172</td>\n",
       "      <td>0.502372</td>\n",
       "      <td>0.685735</td>\n",
       "      <td>0.642152</td>\n",
       "      <td>2.129032</td>\n",
       "      <td>2.045477</td>\n",
       "      <td>0.947982</td>\n",
       "      <td>0.107198</td>\n",
       "      <td>0.734623</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows  21 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Days_Since_Open  Break_Coming  Overnight_Return  Overnight_VIX  \\\n",
       "count      2678.000000   2678.000000       2678.000000    2678.000000   \n",
       "mean          1.444735      0.210978         -0.000022       0.005908   \n",
       "std           0.875799      0.408079          0.001603       0.045081   \n",
       "min           1.000000      0.000000         -0.014194      -0.300366   \n",
       "25%           1.000000      0.000000         -0.000407      -0.016031   \n",
       "50%           1.000000      0.000000         -0.000008       0.000820   \n",
       "75%           1.000000      0.000000          0.000500       0.024785   \n",
       "max           5.000000      1.000000          0.015046       0.510725   \n",
       "\n",
       "            O_to_O  Trail_1d_Ret  Trail_2d_Ret  Trail_3d_Ret  Trail_4d_Ret  \\\n",
       "count  2678.000000   2678.000000   2678.000000   2678.000000   2678.000000   \n",
       "mean      0.000251      0.000260      0.000511      0.000612      0.000847   \n",
       "std       0.012292      0.012973      0.017303      0.020321      0.023078   \n",
       "min      -0.087119     -0.090350     -0.124174     -0.139059     -0.172221   \n",
       "25%      -0.004256     -0.004494     -0.006988     -0.007946     -0.009736   \n",
       "50%       0.000642      0.000569      0.001294      0.002182      0.002630   \n",
       "75%       0.005425      0.005642      0.008789      0.010729      0.012735   \n",
       "max       0.106712      0.115800      0.132064      0.139480      0.179735   \n",
       "\n",
       "       Trail_5d_Ret        ...          Trail_63d_Ret  Trail_126d_Ret  \\\n",
       "count   2678.000000        ...            2678.000000     2678.000000   \n",
       "mean       0.001112        ...               0.013449        0.029453   \n",
       "std        0.025472        ...               0.081302        0.124501   \n",
       "min       -0.183401        ...              -0.417706       -0.464658   \n",
       "25%       -0.010137        ...              -0.022581       -0.027543   \n",
       "50%        0.003111        ...               0.026604        0.043190   \n",
       "75%        0.014399        ...               0.061822        0.097326   \n",
       "max        0.191112        ...               0.388172        0.502372   \n",
       "\n",
       "       Trail_252d_Ret  Trail_1d_VIX  Trail_5d_VIX  Trail_1d_Rel_Vol  \\\n",
       "count     2678.000000   2678.000000   2678.000000       2678.000000   \n",
       "mean         0.059645      0.002947      0.010982          0.008819   \n",
       "std          0.173760      0.074680      0.155039          0.181773   \n",
       "min         -0.488228     -0.295726     -0.426630         -0.832518   \n",
       "25%         -0.004349     -0.040230     -0.079295         -0.090512   \n",
       "50%          0.089677     -0.005022     -0.009048         -0.005982   \n",
       "75%          0.163089      0.036402      0.072292          0.090148   \n",
       "max          0.685735      0.642152      2.129032          2.045477   \n",
       "\n",
       "       Trail_5d_Rel_Vol  Trail_1d_PtT  Trail_1d_VIX_PtT  Intraday_Increase  \n",
       "count       2678.000000   2678.000000       2678.000000        2678.000000  \n",
       "mean           0.010441      0.013533          0.085928           0.547050  \n",
       "std            0.156196      0.011147          0.050513           0.497874  \n",
       "min           -0.669690      0.002009          0.014966           0.000000  \n",
       "25%           -0.083042      0.006880          0.053312           0.000000  \n",
       "50%           -0.000374      0.010471          0.074426           1.000000  \n",
       "75%            0.089807      0.016249          0.103512           1.000000  \n",
       "max            0.947982      0.107198          0.734623           1.000000  \n",
       "\n",
       "[8 rows x 21 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Output basic statistics on the dataset\n",
    "display(data.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Intraday_Increase</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.547050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Intraday_Increase\n",
       "count        2678.000000\n",
       "mean            0.547050\n",
       "std             0.497871\n",
       "min             0.000000\n",
       "25%             0.000000\n",
       "50%             1.000000\n",
       "75%             1.000000\n",
       "max             1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Separate features and labels:\n",
    "\n",
    "X = pd.DataFrame(data.loc[:,'Days_Since_Open':'Trail_1d_VIX_PtT'],dtype='float32')\n",
    "y = pd.DataFrame(data.loc[:,'Intraday_Increase'],dtype='float32')\n",
    "\n",
    "# Summarize the label data\n",
    "display(y.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SKLearn Supervised Learning Algorithms:\n",
    "\n",
    "The next few blocks of code will develop and optimize three SKLearn supervised learning classification algorithms, namely Decision Trees (DT), Naive Bayes, and Support Vector Machines (SVM). This code is based on code from project 2 (student intervention) of the Udacity Machine Learning Nanodegree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Days_Since_Open</th>\n",
       "      <th>Break_Coming</th>\n",
       "      <th>Overnight_Return</th>\n",
       "      <th>Overnight_VIX</th>\n",
       "      <th>O_to_O</th>\n",
       "      <th>Trail_1d_Ret</th>\n",
       "      <th>Trail_2d_Ret</th>\n",
       "      <th>Trail_3d_Ret</th>\n",
       "      <th>Trail_4d_Ret</th>\n",
       "      <th>Trail_5d_Ret</th>\n",
       "      <th>Trail_21d_Ret</th>\n",
       "      <th>Trail_63d_Ret</th>\n",
       "      <th>Trail_126d_Ret</th>\n",
       "      <th>Trail_252d_Ret</th>\n",
       "      <th>Trail_1d_VIX</th>\n",
       "      <th>Trail_5d_VIX</th>\n",
       "      <th>Trail_1d_Rel_Vol</th>\n",
       "      <th>Trail_5d_Rel_Vol</th>\n",
       "      <th>Trail_1d_PtT</th>\n",
       "      <th>Trail_1d_VIX_PtT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "      <td>2426.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.444353</td>\n",
       "      <td>0.211047</td>\n",
       "      <td>-0.000024</td>\n",
       "      <td>0.005527</td>\n",
       "      <td>0.000217</td>\n",
       "      <td>0.000224</td>\n",
       "      <td>0.000441</td>\n",
       "      <td>0.000505</td>\n",
       "      <td>0.000714</td>\n",
       "      <td>0.000955</td>\n",
       "      <td>0.003691</td>\n",
       "      <td>0.013012</td>\n",
       "      <td>0.030937</td>\n",
       "      <td>0.065235</td>\n",
       "      <td>0.003223</td>\n",
       "      <td>0.012115</td>\n",
       "      <td>0.009098</td>\n",
       "      <td>0.011425</td>\n",
       "      <td>0.013841</td>\n",
       "      <td>0.083890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.874623</td>\n",
       "      <td>0.408140</td>\n",
       "      <td>0.001603</td>\n",
       "      <td>0.043615</td>\n",
       "      <td>0.012631</td>\n",
       "      <td>0.013309</td>\n",
       "      <td>0.017728</td>\n",
       "      <td>0.020805</td>\n",
       "      <td>0.023623</td>\n",
       "      <td>0.026056</td>\n",
       "      <td>0.048482</td>\n",
       "      <td>0.083473</td>\n",
       "      <td>0.128763</td>\n",
       "      <td>0.180816</td>\n",
       "      <td>0.073607</td>\n",
       "      <td>0.153743</td>\n",
       "      <td>0.182354</td>\n",
       "      <td>0.158147</td>\n",
       "      <td>0.011501</td>\n",
       "      <td>0.050290</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.014194</td>\n",
       "      <td>-0.300366</td>\n",
       "      <td>-0.087119</td>\n",
       "      <td>-0.090350</td>\n",
       "      <td>-0.124174</td>\n",
       "      <td>-0.139059</td>\n",
       "      <td>-0.172221</td>\n",
       "      <td>-0.183401</td>\n",
       "      <td>-0.280077</td>\n",
       "      <td>-0.417706</td>\n",
       "      <td>-0.464658</td>\n",
       "      <td>-0.488228</td>\n",
       "      <td>-0.295727</td>\n",
       "      <td>-0.393590</td>\n",
       "      <td>-0.832518</td>\n",
       "      <td>-0.669690</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.014966</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000342</td>\n",
       "      <td>-0.015761</td>\n",
       "      <td>-0.004302</td>\n",
       "      <td>-0.004533</td>\n",
       "      <td>-0.007193</td>\n",
       "      <td>-0.008456</td>\n",
       "      <td>-0.010041</td>\n",
       "      <td>-0.010553</td>\n",
       "      <td>-0.018995</td>\n",
       "      <td>-0.022675</td>\n",
       "      <td>-0.027350</td>\n",
       "      <td>0.004498</td>\n",
       "      <td>-0.039352</td>\n",
       "      <td>-0.076418</td>\n",
       "      <td>-0.090298</td>\n",
       "      <td>-0.082820</td>\n",
       "      <td>0.007032</td>\n",
       "      <td>0.052200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.000008</td>\n",
       "      <td>0.000790</td>\n",
       "      <td>0.000642</td>\n",
       "      <td>0.000622</td>\n",
       "      <td>0.001292</td>\n",
       "      <td>0.002193</td>\n",
       "      <td>0.002646</td>\n",
       "      <td>0.003077</td>\n",
       "      <td>0.011066</td>\n",
       "      <td>0.026493</td>\n",
       "      <td>0.048630</td>\n",
       "      <td>0.104952</td>\n",
       "      <td>-0.004886</td>\n",
       "      <td>-0.007810</td>\n",
       "      <td>-0.005607</td>\n",
       "      <td>0.000701</td>\n",
       "      <td>0.010627</td>\n",
       "      <td>0.072389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000466</td>\n",
       "      <td>0.024386</td>\n",
       "      <td>0.005476</td>\n",
       "      <td>0.005689</td>\n",
       "      <td>0.008879</td>\n",
       "      <td>0.010794</td>\n",
       "      <td>0.012735</td>\n",
       "      <td>0.014443</td>\n",
       "      <td>0.031853</td>\n",
       "      <td>0.062510</td>\n",
       "      <td>0.101349</td>\n",
       "      <td>0.173855</td>\n",
       "      <td>0.035583</td>\n",
       "      <td>0.072183</td>\n",
       "      <td>0.091046</td>\n",
       "      <td>0.092600</td>\n",
       "      <td>0.016484</td>\n",
       "      <td>0.100435</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.015046</td>\n",
       "      <td>0.292127</td>\n",
       "      <td>0.106712</td>\n",
       "      <td>0.115800</td>\n",
       "      <td>0.132064</td>\n",
       "      <td>0.139480</td>\n",
       "      <td>0.179735</td>\n",
       "      <td>0.191112</td>\n",
       "      <td>0.234342</td>\n",
       "      <td>0.388172</td>\n",
       "      <td>0.502372</td>\n",
       "      <td>0.685735</td>\n",
       "      <td>0.642152</td>\n",
       "      <td>2.129032</td>\n",
       "      <td>2.045477</td>\n",
       "      <td>0.947982</td>\n",
       "      <td>0.107198</td>\n",
       "      <td>0.734623</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Days_Since_Open  Break_Coming  Overnight_Return  Overnight_VIX  \\\n",
       "count      2426.000000   2426.000000       2426.000000    2426.000000   \n",
       "mean          1.444353      0.211047         -0.000024       0.005527   \n",
       "std           0.874623      0.408140          0.001603       0.043615   \n",
       "min           1.000000      0.000000         -0.014194      -0.300366   \n",
       "25%           1.000000      0.000000         -0.000342      -0.015761   \n",
       "50%           1.000000      0.000000         -0.000008       0.000790   \n",
       "75%           1.000000      0.000000          0.000466       0.024386   \n",
       "max           5.000000      1.000000          0.015046       0.292127   \n",
       "\n",
       "            O_to_O  Trail_1d_Ret  Trail_2d_Ret  Trail_3d_Ret  Trail_4d_Ret  \\\n",
       "count  2426.000000   2426.000000   2426.000000   2426.000000   2426.000000   \n",
       "mean      0.000217      0.000224      0.000441      0.000505      0.000714   \n",
       "std       0.012631      0.013309      0.017728      0.020805      0.023623   \n",
       "min      -0.087119     -0.090350     -0.124174     -0.139059     -0.172221   \n",
       "25%      -0.004302     -0.004533     -0.007193     -0.008456     -0.010041   \n",
       "50%       0.000642      0.000622      0.001292      0.002193      0.002646   \n",
       "75%       0.005476      0.005689      0.008879      0.010794      0.012735   \n",
       "max       0.106712      0.115800      0.132064      0.139480      0.179735   \n",
       "\n",
       "       Trail_5d_Ret  Trail_21d_Ret  Trail_63d_Ret  Trail_126d_Ret  \\\n",
       "count   2426.000000    2426.000000    2426.000000     2426.000000   \n",
       "mean       0.000955       0.003691       0.013012        0.030937   \n",
       "std        0.026056       0.048482       0.083473        0.128763   \n",
       "min       -0.183401      -0.280077      -0.417706       -0.464658   \n",
       "25%       -0.010553      -0.018995      -0.022675       -0.027350   \n",
       "50%        0.003077       0.011066       0.026493        0.048630   \n",
       "75%        0.014443       0.031853       0.062510        0.101349   \n",
       "max        0.191112       0.234342       0.388172        0.502372   \n",
       "\n",
       "       Trail_252d_Ret  Trail_1d_VIX  Trail_5d_VIX  Trail_1d_Rel_Vol  \\\n",
       "count     2426.000000   2426.000000   2426.000000       2426.000000   \n",
       "mean         0.065235      0.003223      0.012115          0.009098   \n",
       "std          0.180816      0.073607      0.153743          0.182354   \n",
       "min         -0.488228     -0.295727     -0.393590         -0.832518   \n",
       "25%          0.004498     -0.039352     -0.076418         -0.090298   \n",
       "50%          0.104952     -0.004886     -0.007810         -0.005607   \n",
       "75%          0.173855      0.035583      0.072183          0.091046   \n",
       "max          0.685735      0.642152      2.129032          2.045477   \n",
       "\n",
       "       Trail_5d_Rel_Vol  Trail_1d_PtT  Trail_1d_VIX_PtT  \n",
       "count       2426.000000   2426.000000       2426.000000  \n",
       "mean           0.011425      0.013841          0.083890  \n",
       "std            0.158147      0.011501          0.050290  \n",
       "min           -0.669690      0.002009          0.014966  \n",
       "25%           -0.082820      0.007032          0.052200  \n",
       "50%            0.000701      0.010627          0.072389  \n",
       "75%            0.092600      0.016484          0.100435  \n",
       "max            0.947982      0.107198          0.734623  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Days_Since_Open</th>\n",
       "      <th>Break_Coming</th>\n",
       "      <th>Overnight_Return</th>\n",
       "      <th>Overnight_VIX</th>\n",
       "      <th>O_to_O</th>\n",
       "      <th>Trail_1d_Ret</th>\n",
       "      <th>Trail_2d_Ret</th>\n",
       "      <th>Trail_3d_Ret</th>\n",
       "      <th>Trail_4d_Ret</th>\n",
       "      <th>Trail_5d_Ret</th>\n",
       "      <th>Trail_21d_Ret</th>\n",
       "      <th>Trail_63d_Ret</th>\n",
       "      <th>Trail_126d_Ret</th>\n",
       "      <th>Trail_252d_Ret</th>\n",
       "      <th>Trail_1d_VIX</th>\n",
       "      <th>Trail_5d_VIX</th>\n",
       "      <th>Trail_1d_Rel_Vol</th>\n",
       "      <th>Trail_5d_Rel_Vol</th>\n",
       "      <th>Trail_1d_PtT</th>\n",
       "      <th>Trail_1d_VIX_PtT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>2.520000e+02</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>1.448413</td>\n",
       "      <td>0.210317</td>\n",
       "      <td>4.407181e-07</td>\n",
       "      <td>0.009577</td>\n",
       "      <td>0.000587</td>\n",
       "      <td>0.000605</td>\n",
       "      <td>0.001189</td>\n",
       "      <td>0.001640</td>\n",
       "      <td>0.002124</td>\n",
       "      <td>0.002622</td>\n",
       "      <td>0.009352</td>\n",
       "      <td>0.017652</td>\n",
       "      <td>0.015159</td>\n",
       "      <td>0.005831</td>\n",
       "      <td>0.000298</td>\n",
       "      <td>0.000078</td>\n",
       "      <td>0.006127</td>\n",
       "      <td>0.000971</td>\n",
       "      <td>0.010563</td>\n",
       "      <td>0.105551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.888909</td>\n",
       "      <td>0.408345</td>\n",
       "      <td>1.604419e-03</td>\n",
       "      <td>0.057292</td>\n",
       "      <td>0.008363</td>\n",
       "      <td>0.009143</td>\n",
       "      <td>0.012497</td>\n",
       "      <td>0.014866</td>\n",
       "      <td>0.016942</td>\n",
       "      <td>0.018933</td>\n",
       "      <td>0.037278</td>\n",
       "      <td>0.056219</td>\n",
       "      <td>0.070058</td>\n",
       "      <td>0.054138</td>\n",
       "      <td>0.084439</td>\n",
       "      <td>0.166937</td>\n",
       "      <td>0.176414</td>\n",
       "      <td>0.135903</td>\n",
       "      <td>0.006132</td>\n",
       "      <td>0.048502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-5.602164e-03</td>\n",
       "      <td>-0.102524</td>\n",
       "      <td>-0.034395</td>\n",
       "      <td>-0.035920</td>\n",
       "      <td>-0.053366</td>\n",
       "      <td>-0.046948</td>\n",
       "      <td>-0.049341</td>\n",
       "      <td>-0.059645</td>\n",
       "      <td>-0.092491</td>\n",
       "      <td>-0.121361</td>\n",
       "      <td>-0.123185</td>\n",
       "      <td>-0.115759</td>\n",
       "      <td>-0.213836</td>\n",
       "      <td>-0.426630</td>\n",
       "      <td>-0.676333</td>\n",
       "      <td>-0.430812</td>\n",
       "      <td>0.002597</td>\n",
       "      <td>0.031952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-1.001223e-03</td>\n",
       "      <td>-0.017472</td>\n",
       "      <td>-0.003650</td>\n",
       "      <td>-0.003330</td>\n",
       "      <td>-0.004971</td>\n",
       "      <td>-0.006011</td>\n",
       "      <td>-0.006977</td>\n",
       "      <td>-0.006892</td>\n",
       "      <td>-0.011080</td>\n",
       "      <td>-0.021088</td>\n",
       "      <td>-0.027548</td>\n",
       "      <td>-0.023973</td>\n",
       "      <td>-0.053735</td>\n",
       "      <td>-0.113056</td>\n",
       "      <td>-0.093940</td>\n",
       "      <td>-0.085283</td>\n",
       "      <td>0.006028</td>\n",
       "      <td>0.073073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>6.708986e-05</td>\n",
       "      <td>0.002482</td>\n",
       "      <td>0.000573</td>\n",
       "      <td>0.000209</td>\n",
       "      <td>0.001337</td>\n",
       "      <td>0.001823</td>\n",
       "      <td>0.002451</td>\n",
       "      <td>0.003944</td>\n",
       "      <td>0.009779</td>\n",
       "      <td>0.029720</td>\n",
       "      <td>0.006411</td>\n",
       "      <td>-0.002416</td>\n",
       "      <td>-0.008548</td>\n",
       "      <td>-0.021378</td>\n",
       "      <td>-0.008384</td>\n",
       "      <td>-0.012706</td>\n",
       "      <td>0.008984</td>\n",
       "      <td>0.094671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>9.068254e-04</td>\n",
       "      <td>0.028720</td>\n",
       "      <td>0.004812</td>\n",
       "      <td>0.005236</td>\n",
       "      <td>0.007628</td>\n",
       "      <td>0.009715</td>\n",
       "      <td>0.012169</td>\n",
       "      <td>0.013917</td>\n",
       "      <td>0.031173</td>\n",
       "      <td>0.059052</td>\n",
       "      <td>0.047859</td>\n",
       "      <td>0.034760</td>\n",
       "      <td>0.044724</td>\n",
       "      <td>0.073382</td>\n",
       "      <td>0.084300</td>\n",
       "      <td>0.076359</td>\n",
       "      <td>0.013615</td>\n",
       "      <td>0.125867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>4.499757e-03</td>\n",
       "      <td>0.510725</td>\n",
       "      <td>0.022672</td>\n",
       "      <td>0.024760</td>\n",
       "      <td>0.036357</td>\n",
       "      <td>0.053437</td>\n",
       "      <td>0.054647</td>\n",
       "      <td>0.055947</td>\n",
       "      <td>0.109475</td>\n",
       "      <td>0.130730</td>\n",
       "      <td>0.195022</td>\n",
       "      <td>0.164826</td>\n",
       "      <td>0.493333</td>\n",
       "      <td>0.646860</td>\n",
       "      <td>1.308129</td>\n",
       "      <td>0.466593</td>\n",
       "      <td>0.034405</td>\n",
       "      <td>0.334444</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Days_Since_Open  Break_Coming  Overnight_Return  Overnight_VIX  \\\n",
       "count       252.000000    252.000000      2.520000e+02     252.000000   \n",
       "mean          1.448413      0.210317      4.407181e-07       0.009577   \n",
       "std           0.888909      0.408345      1.604419e-03       0.057292   \n",
       "min           1.000000      0.000000     -5.602164e-03      -0.102524   \n",
       "25%           1.000000      0.000000     -1.001223e-03      -0.017472   \n",
       "50%           1.000000      0.000000      6.708986e-05       0.002482   \n",
       "75%           1.000000      0.000000      9.068254e-04       0.028720   \n",
       "max           4.000000      1.000000      4.499757e-03       0.510725   \n",
       "\n",
       "           O_to_O  Trail_1d_Ret  Trail_2d_Ret  Trail_3d_Ret  Trail_4d_Ret  \\\n",
       "count  252.000000    252.000000    252.000000    252.000000    252.000000   \n",
       "mean     0.000587      0.000605      0.001189      0.001640      0.002124   \n",
       "std      0.008363      0.009143      0.012497      0.014866      0.016942   \n",
       "min     -0.034395     -0.035920     -0.053366     -0.046948     -0.049341   \n",
       "25%     -0.003650     -0.003330     -0.004971     -0.006011     -0.006977   \n",
       "50%      0.000573      0.000209      0.001337      0.001823      0.002451   \n",
       "75%      0.004812      0.005236      0.007628      0.009715      0.012169   \n",
       "max      0.022672      0.024760      0.036357      0.053437      0.054647   \n",
       "\n",
       "       Trail_5d_Ret  Trail_21d_Ret  Trail_63d_Ret  Trail_126d_Ret  \\\n",
       "count    252.000000     252.000000     252.000000      252.000000   \n",
       "mean       0.002622       0.009352       0.017652        0.015159   \n",
       "std        0.018933       0.037278       0.056219        0.070058   \n",
       "min       -0.059645      -0.092491      -0.121361       -0.123185   \n",
       "25%       -0.006892      -0.011080      -0.021088       -0.027548   \n",
       "50%        0.003944       0.009779       0.029720        0.006411   \n",
       "75%        0.013917       0.031173       0.059052        0.047859   \n",
       "max        0.055947       0.109475       0.130730        0.195022   \n",
       "\n",
       "       Trail_252d_Ret  Trail_1d_VIX  Trail_5d_VIX  Trail_1d_Rel_Vol  \\\n",
       "count      252.000000    252.000000    252.000000        252.000000   \n",
       "mean         0.005831      0.000298      0.000078          0.006127   \n",
       "std          0.054138      0.084439      0.166937          0.176414   \n",
       "min         -0.115759     -0.213836     -0.426630         -0.676333   \n",
       "25%         -0.023973     -0.053735     -0.113056         -0.093940   \n",
       "50%         -0.002416     -0.008548     -0.021378         -0.008384   \n",
       "75%          0.034760      0.044724      0.073382          0.084300   \n",
       "max          0.164826      0.493333      0.646860          1.308129   \n",
       "\n",
       "       Trail_5d_Rel_Vol  Trail_1d_PtT  Trail_1d_VIX_PtT  \n",
       "count        252.000000    252.000000        252.000000  \n",
       "mean           0.000971      0.010563          0.105551  \n",
       "std            0.135903      0.006132          0.048502  \n",
       "min           -0.430812      0.002597          0.031952  \n",
       "25%           -0.085283      0.006028          0.073073  \n",
       "50%           -0.012706      0.008984          0.094671  \n",
       "75%            0.076359      0.013615          0.125867  \n",
       "max            0.466593      0.034405          0.334444  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2426.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.549052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497694</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0\n",
       "count  2426.000000\n",
       "mean      0.549052\n",
       "std       0.497694\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       1.000000\n",
       "75%       1.000000\n",
       "max       1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0\n",
       "count  252.000000\n",
       "mean     0.527778\n",
       "std      0.500221\n",
       "min      0.000000\n",
       "25%      0.000000\n",
       "50%      1.000000\n",
       "75%      1.000000\n",
       "max      1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data into train, validation and test sets:\n",
    "# Shuffling will not be used because test data should be drawn from the latest data points, to remove any risk of\n",
    "# the learning algorithms glimpsing the future\n",
    "\n",
    "X_train = X[252:]\n",
    "X_test = X[:252]\n",
    "\n",
    "y_train = y[252:]\n",
    "y_test = y[:252]\n",
    "\n",
    "y_train = np.reshape(y_train.values,[2426,])\n",
    "y_test = np.reshape(y_test.values,[252,])\n",
    "\n",
    "describe_train = pd.DataFrame(X_train)\n",
    "describe_test = pd.DataFrame(X_test)\n",
    "\n",
    "describe_train_labels = pd.DataFrame(y_train)\n",
    "describe_test_labels = pd.DataFrame(y_test)\n",
    "\n",
    "\n",
    "display(describe_train.describe())\n",
    "display(describe_test.describe())\n",
    "\n",
    "display(describe_train_labels.describe())\n",
    "display(describe_test_labels.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score for predicting all \"up (1)\" on test set: 0.6909\n",
      "Accuracy score for predicting all \"up (1)\" on test set: 0.5278\n"
     ]
    }
   ],
   "source": [
    "# Calculate the benchmark values, against which the learners will be assessed\n",
    "\n",
    "# A suitable benchmark for the models would be comparing to the case where one simply predicts all 1s on the test set\n",
    "# i.e. simply assume the market will go up every day, as it historically has on 52-54% of days in the time range\n",
    "# under consideration here:\n",
    "\n",
    "# Benchmark F1 score results from simply predicting all '1' on the test set\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "print (\"F1 score for predicting all \\\"up (1)\\\" on test set: {:.4f}\".format(\n",
    "    f1_score(y_test, [1]*len(y_test), pos_label=1, average='binary')))\n",
    "\n",
    "# Benchmark accuracy score results from simply predicting all '1' on the test set\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print (\"Accuracy score for predicting all \\\"up (1)\\\" on test set: {:.4f}\".format(\n",
    "    accuracy_score(y_test, [1]*len(y_test))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set up a series of functions for monitoring training and testing of the various learning algorithms\n",
    "\n",
    "def train_classifier(clf, X_train, y_train):\n",
    "    ''' Fits a classifier to the training data. '''\n",
    "    \n",
    "    # Start the clock, train the classifier, then stop the clock\n",
    "    start = time()\n",
    "    clf.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    \n",
    "    # Print the results\n",
    "    print (\"Trained model in {:.4f} seconds\".format(end - start))\n",
    "\n",
    "def predict_labels(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on F1 score. '''\n",
    "    \n",
    "    # Start the clock, make predictions, then stop the clock\n",
    "    start = time()\n",
    "    y_pred = clf.predict(features)\n",
    "    end = time()\n",
    "    \n",
    "    # Print and return results\n",
    "    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n",
    "    return f1_score(target, y_pred, pos_label=1)\n",
    "\n",
    "def predict_labels_accuracy(clf, features, target):\n",
    "    ''' Makes predictions using a fit classifier based on the accuracy score. '''\n",
    "    \n",
    "    y_pred = clf.predict(features)\n",
    "    \n",
    "    # Print and return results\n",
    "    return accuracy_score(target, y_pred)   \n",
    "\n",
    "def train_predict(clf, X_train, y_train, X_test, y_test):\n",
    "    ''' Train and predict using a classifer based on F1 score. '''\n",
    "    \n",
    "    # Indicate the classifier and the training set size\n",
    "    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, len(X_train)))\n",
    "    \n",
    "    # Train the classifier\n",
    "    train_classifier(clf, X_train, y_train)\n",
    "    \n",
    "    # Print the results of prediction for both training and testing\n",
    "    print (\"F1 score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "    print (\"Accuracy score for training set: {:.4f}.\".format(predict_labels_accuracy(clf, X_train, y_train)))\n",
    "    print (\"F1 score for test set: {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "    print (\"Accuracy score for test set: {:.4f}.\".format(predict_labels_accuracy(clf, X_test, y_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DecisionTreeClassifier: \n",
      "\n",
      "Training a DecisionTreeClassifier using a training set size of 800. . .\n",
      "Trained model in 0.0128 seconds\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0001 seconds.\n",
      "F1 score for test set: 0.5292.\n",
      "Accuracy score for test set: 0.5198.\n",
      "--------------------------------\n",
      "Training a DecisionTreeClassifier using a training set size of 1600. . .\n",
      "Trained model in 0.0243 seconds\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0001 seconds.\n",
      "F1 score for test set: 0.6043.\n",
      "Accuracy score for test set: 0.5635.\n",
      "--------------------------------\n",
      "Training a DecisionTreeClassifier using a training set size of 2426. . .\n",
      "Trained model in 0.0418 seconds\n",
      "Made predictions in 0.0004 seconds.\n",
      "F1 score for training set: 1.0000.\n",
      "Accuracy score for training set: 1.0000.\n",
      "Made predictions in 0.0001 seconds.\n",
      "F1 score for test set: 0.5827.\n",
      "Accuracy score for test set: 0.5397.\n",
      "--------------------------------\n",
      "\n",
      "GaussianNB: \n",
      "\n",
      "Training a GaussianNB using a training set size of 800. . .\n",
      "Trained model in 0.0007 seconds\n",
      "Made predictions in 0.0004 seconds.\n",
      "F1 score for training set: 0.7220.\n",
      "Accuracy score for training set: 0.6650.\n",
      "Made predictions in 0.0002 seconds.\n",
      "F1 score for test set: 0.6400.\n",
      "Accuracy score for test set: 0.6071.\n",
      "--------------------------------\n",
      "Training a GaussianNB using a training set size of 1600. . .\n",
      "Trained model in 0.0009 seconds\n",
      "Made predictions in 0.0008 seconds.\n",
      "F1 score for training set: 0.7073.\n",
      "Accuracy score for training set: 0.6219.\n",
      "Made predictions in 0.0002 seconds.\n",
      "F1 score for test set: 0.6543.\n",
      "Accuracy score for test set: 0.5556.\n",
      "--------------------------------\n",
      "Training a GaussianNB using a training set size of 2426. . .\n",
      "Trained model in 0.0010 seconds\n",
      "Made predictions in 0.0010 seconds.\n",
      "F1 score for training set: 0.7021.\n",
      "Accuracy score for training set: 0.6146.\n",
      "Made predictions in 0.0003 seconds.\n",
      "F1 score for test set: 0.6812.\n",
      "Accuracy score for test set: 0.5952.\n",
      "--------------------------------\n",
      "\n",
      "SVC: \n",
      "\n",
      "Training a SVC using a training set size of 800. . .\n",
      "Trained model in 0.0260 seconds\n",
      "Made predictions in 0.0200 seconds.\n",
      "F1 score for training set: 0.7045.\n",
      "Accuracy score for training set: 0.5437.\n",
      "Made predictions in 0.0064 seconds.\n",
      "F1 score for test set: 0.6909.\n",
      "Accuracy score for test set: 0.5278.\n",
      "--------------------------------\n",
      "Training a SVC using a training set size of 1600. . .\n",
      "Trained model in 0.1076 seconds\n",
      "Made predictions in 0.0836 seconds.\n",
      "F1 score for training set: 0.7081.\n",
      "Accuracy score for training set: 0.5481.\n",
      "Made predictions in 0.0136 seconds.\n",
      "F1 score for test set: 0.6909.\n",
      "Accuracy score for test set: 0.5278.\n",
      "--------------------------------\n",
      "Training a SVC using a training set size of 2426. . .\n",
      "Trained model in 0.2761 seconds\n",
      "Made predictions in 0.1751 seconds.\n",
      "F1 score for training set: 0.7089.\n",
      "Accuracy score for training set: 0.5491.\n",
      "Made predictions in 0.0229 seconds.\n",
      "F1 score for test set: 0.6909.\n",
      "Accuracy score for test set: 0.5278.\n",
      "--------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Import  three supervised learning models from sklearn\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Initialize the three models\n",
    "clf_A = DecisionTreeClassifier(random_state=1)\n",
    "clf_B = GaussianNB()\n",
    "clf_C = SVC(random_state=2)\n",
    "\n",
    "# Set up the training set sizes\n",
    "X_train_800 = X_train[:800]\n",
    "y_train_800 = y_train[:800]\n",
    "\n",
    "X_train_1600 = X_train[:1600]\n",
    "y_train_1600 = y_train[:1600]\n",
    "\n",
    "X_train_2426 = X_train[:2426]\n",
    "y_train_2426 = y_train[:2426]\n",
    "\n",
    "# Execute the 'train_predict' function for each classifier and each training set size\n",
    "\n",
    "# loop thru models, then thru train sizes\n",
    "for clf in [clf_A, clf_B, clf_C]:\n",
    "    print (\"\\n{}: \\n\".format(clf.__class__.__name__))\n",
    "    for n in [800, 1600, 2426]:\n",
    "        train_predict(clf, X_train[:n], y_train[:n], X_test, y_test)\n",
    "        print ('-'*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.0002 seconds.\n",
      "Tuned model has a training F1 score of 0.6680.\n",
      "Tuned model has a training accuracy score of 0.6533.\n",
      "Made predictions in 0.0001 seconds.\n",
      "Tuned model has a testing F1 score of 0.6763.\n",
      "Tuned model has a testing accuracy score of 0.6429.\n",
      "The best parameters are {'splitter': 'best', 'criterion': 'gini', 'max_depth': 3}\n",
      "\n",
      "\n",
      "Feature Importance:\n",
      "Days_Since_Open     0.000000\n",
      "Break_Coming        0.000000\n",
      "Overnight_Return    0.574240\n",
      "Overnight_VIX       0.384663\n",
      "O_to_O              0.000000\n",
      "Trail_1d_Ret        0.000000\n",
      "Trail_2d_Ret        0.000000\n",
      "Trail_3d_Ret        0.000000\n",
      "Trail_4d_Ret        0.000000\n",
      "Trail_5d_Ret        0.000000\n",
      "Trail_21d_Ret       0.025981\n",
      "Trail_63d_Ret       0.015116\n",
      "Trail_126d_Ret      0.000000\n",
      "Trail_252d_Ret      0.000000\n",
      "Trail_1d_VIX        0.000000\n",
      "Trail_5d_VIX        0.000000\n",
      "Trail_1d_Rel_Vol    0.000000\n",
      "Trail_5d_Rel_Vol    0.000000\n",
      "Trail_1d_PtT        0.000000\n",
      "Trail_1d_VIX_PtT    0.000000\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Optimize the Decision Tree parameters by using grid search cross validation\n",
    "\n",
    "# Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "\n",
    "parameters = {'max_depth': [1,2,3,4,5,6,7,8,9],'criterion':('gini','entropy'),'splitter':('best','random')}\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = DecisionTreeClassifier(random_state=4)\n",
    "\n",
    "# Perform grid search on the classifier using the default scoring method (accuracy)\n",
    "grid_obj = GridSearchCV(clf, parameters)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj.fit(X_train,y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a training accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "print (\"Tuned model has a testing accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_test, y_test)))\n",
    "\n",
    "print(\"The best parameters are %s\"\n",
    "      % (grid_obj.best_params_))\n",
    "\n",
    "importances = clf.feature_importances_\n",
    "\n",
    "feature_importance = pd.Series(importances,index=['Days_Since_Open','Break_Coming','Overnight_Return',\n",
    "                                                  'Overnight_VIX','O_to_O','Trail_1d_Ret','Trail_2d_Ret',\n",
    "                                                  'Trail_3d_Ret','Trail_4d_Ret','Trail_5d_Ret','Trail_21d_Ret',\n",
    "                                                  'Trail_63d_Ret','Trail_126d_Ret','Trail_252d_Ret','Trail_1d_VIX',\n",
    "                                                  'Trail_5d_VIX','Trail_1d_Rel_Vol','Trail_5d_Rel_Vol','Trail_1d_PtT',\n",
    "                                                  'Trail_1d_VIX_PtT'])\n",
    "\n",
    "print '\\n'\n",
    "print \"Feature Importance:\" \n",
    "print feature_importance\n",
    "\n",
    "# Output predictions for further analysis\n",
    "np.savetxt(\"eval-DT.csv\", clf.predict(X_test), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Made predictions in 0.1151 seconds.\n",
      "Tuned model has a training F1 score of 0.7250.\n",
      "Tuned model has a training accuracy score of 0.6385.\n",
      "Made predictions in 0.0128 seconds.\n",
      "Tuned model has a testing F1 score of 0.7032.\n",
      "Tuned model has a testing accuracy score of 0.6349.\n",
      "The best parameters are {'kernel': 'linear', 'C': 12, 'gamma': 1e-06}\n"
     ]
    }
   ],
   "source": [
    "# Optimize the SVM parameters by using grid search cross validation\n",
    "\n",
    "# Import 'GridSearchCV' and 'make_scorer'\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "# Create the parameters list you wish to tune\n",
    "\n",
    "parameters = {'kernel':('linear', 'rbf','sigmoid'), 'C':[1,2,3,4,5,6,7,8,9,10,11,12,13,14,15], \n",
    "              'gamma':[.000001,.000005,.00005,.0005,.001,.005,.01,.02,.04,.05,.1,.3,.5]}\n",
    "\n",
    "# Initialize the classifier\n",
    "clf = SVC(random_state=3)\n",
    "\n",
    "# Perform grid search on the classifier using the default scoring method (accuracy)\n",
    "grid_obj = GridSearchCV(clf, parameters)\n",
    "\n",
    "# Fit the grid search object to the training data and find the optimal parameters\n",
    "grid_obj.fit(X_train,y_train)\n",
    "\n",
    "# Get the estimator\n",
    "clf = grid_obj.best_estimator_\n",
    "\n",
    "# Report the final F1 score for training and testing after parameter tuning\n",
    "print (\"Tuned model has a training F1 score of {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a training accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_train, y_train)))\n",
    "print (\"Tuned model has a testing F1 score of {:.4f}.\".format(predict_labels(clf, X_test, y_test)))\n",
    "print (\"Tuned model has a testing accuracy score of {:.4f}.\".format(predict_labels_accuracy(clf, X_test, y_test)))\n",
    "\n",
    "print(\"The best parameters are %s\"\n",
    "      % (grid_obj.best_params_))\n",
    "\n",
    "# Output predictions for further analysis\n",
    "np.savetxt(\"eval-SVM.csv\", clf.predict(X_test), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network Algorithm:\n",
    "\n",
    "The next few blocks of code will develop and optimize a TensorFlow based neural network. This code is based on the assignments from the Udacity 'Deep Learning' course."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Scale features (mean = 0, stdev = 1):\n",
    "\n",
    "from sklearn import preprocessing\n",
    "X_scaled = preprocessing.scale(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>2678.000000</td>\n",
       "      <td>2678.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.452950</td>\n",
       "      <td>0.547050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.497874</td>\n",
       "      <td>0.497874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1\n",
       "count  2678.000000  2678.000000\n",
       "mean      0.452950     0.547050\n",
       "std       0.497874     0.497874\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       0.000000     1.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(2678, 2)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1-hot encoding on labels (y):\n",
    "\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "enc = OneHotEncoder()\n",
    "enc.fit(y)\n",
    "y_scaled = enc.transform(y).toarray()\n",
    "\n",
    "y_insight = pd.DataFrame(y_scaled)\n",
    "\n",
    "display(y_insight.describe())\n",
    "\n",
    "y_scaled.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set', (1922, 20), (1922, 2))\n",
      "('Validation set', (504, 20), (504, 2))\n",
      "('Test set', (252, 20), (252, 2))\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-0.002246</td>\n",
       "      <td>-0.004464</td>\n",
       "      <td>-0.022735</td>\n",
       "      <td>-0.017680</td>\n",
       "      <td>-0.003585</td>\n",
       "      <td>-0.003335</td>\n",
       "      <td>-0.004705</td>\n",
       "      <td>-0.008693</td>\n",
       "      <td>-0.009718</td>\n",
       "      <td>-0.009922</td>\n",
       "      <td>-0.027411</td>\n",
       "      <td>-0.043538</td>\n",
       "      <td>-0.049059</td>\n",
       "      <td>-0.098692</td>\n",
       "      <td>0.000492</td>\n",
       "      <td>-0.008035</td>\n",
       "      <td>0.002379</td>\n",
       "      <td>0.010455</td>\n",
       "      <td>0.133389</td>\n",
       "      <td>-0.081174</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.999763</td>\n",
       "      <td>0.997082</td>\n",
       "      <td>1.074310</td>\n",
       "      <td>0.923680</td>\n",
       "      <td>1.109777</td>\n",
       "      <td>1.106825</td>\n",
       "      <td>1.098113</td>\n",
       "      <td>1.092914</td>\n",
       "      <td>1.091955</td>\n",
       "      <td>1.093523</td>\n",
       "      <td>1.101376</td>\n",
       "      <td>1.127321</td>\n",
       "      <td>1.140101</td>\n",
       "      <td>1.113726</td>\n",
       "      <td>0.960511</td>\n",
       "      <td>0.880923</td>\n",
       "      <td>1.021252</td>\n",
       "      <td>1.048842</td>\n",
       "      <td>1.105402</td>\n",
       "      <td>0.952326</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>-8.844321</td>\n",
       "      <td>-6.795061</td>\n",
       "      <td>-7.109165</td>\n",
       "      <td>-6.985810</td>\n",
       "      <td>-7.207462</td>\n",
       "      <td>-6.874485</td>\n",
       "      <td>-7.500630</td>\n",
       "      <td>-7.245063</td>\n",
       "      <td>-5.978411</td>\n",
       "      <td>-5.304133</td>\n",
       "      <td>-3.969464</td>\n",
       "      <td>-3.153637</td>\n",
       "      <td>-4.000111</td>\n",
       "      <td>-2.340529</td>\n",
       "      <td>-4.223995</td>\n",
       "      <td>-4.355154</td>\n",
       "      <td>-0.990167</td>\n",
       "      <td>-1.405074</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>-0.170724</td>\n",
       "      <td>-0.461074</td>\n",
       "      <td>-0.396309</td>\n",
       "      <td>-0.401901</td>\n",
       "      <td>-0.478210</td>\n",
       "      <td>-0.497041</td>\n",
       "      <td>-0.523534</td>\n",
       "      <td>-0.501545</td>\n",
       "      <td>-0.569747</td>\n",
       "      <td>-0.603210</td>\n",
       "      <td>-0.569127</td>\n",
       "      <td>-0.455262</td>\n",
       "      <td>-0.556414</td>\n",
       "      <td>-0.554569</td>\n",
       "      <td>-0.559025</td>\n",
       "      <td>-0.600749</td>\n",
       "      <td>-0.535700</td>\n",
       "      <td>-0.687520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>0.004919</td>\n",
       "      <td>-0.122359</td>\n",
       "      <td>0.036526</td>\n",
       "      <td>0.032513</td>\n",
       "      <td>0.041032</td>\n",
       "      <td>0.077633</td>\n",
       "      <td>0.085518</td>\n",
       "      <td>0.074873</td>\n",
       "      <td>0.171047</td>\n",
       "      <td>0.136405</td>\n",
       "      <td>0.051890</td>\n",
       "      <td>0.074842</td>\n",
       "      <td>-0.107183</td>\n",
       "      <td>-0.124521</td>\n",
       "      <td>-0.076691</td>\n",
       "      <td>-0.051002</td>\n",
       "      <td>-0.167506</td>\n",
       "      <td>-0.298434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>0.223773</td>\n",
       "      <td>0.390256</td>\n",
       "      <td>0.462439</td>\n",
       "      <td>0.446846</td>\n",
       "      <td>0.522221</td>\n",
       "      <td>0.536587</td>\n",
       "      <td>0.552471</td>\n",
       "      <td>0.561334</td>\n",
       "      <td>0.619494</td>\n",
       "      <td>0.705675</td>\n",
       "      <td>0.662916</td>\n",
       "      <td>0.559837</td>\n",
       "      <td>0.408253</td>\n",
       "      <td>0.387349</td>\n",
       "      <td>0.469229</td>\n",
       "      <td>0.528372</td>\n",
       "      <td>0.402930</td>\n",
       "      <td>0.234408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.060214</td>\n",
       "      <td>1.933862</td>\n",
       "      <td>9.403597</td>\n",
       "      <td>6.350112</td>\n",
       "      <td>8.662504</td>\n",
       "      <td>8.907937</td>\n",
       "      <td>7.604455</td>\n",
       "      <td>6.834938</td>\n",
       "      <td>7.752933</td>\n",
       "      <td>7.460513</td>\n",
       "      <td>4.839027</td>\n",
       "      <td>4.609906</td>\n",
       "      <td>3.799225</td>\n",
       "      <td>3.603865</td>\n",
       "      <td>8.560812</td>\n",
       "      <td>6.565746</td>\n",
       "      <td>11.206517</td>\n",
       "      <td>6.003443</td>\n",
       "      <td>8.404070</td>\n",
       "      <td>9.285350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0            1            2            3            4   \\\n",
       "count  1922.000000  1922.000000  1922.000000  1922.000000  1922.000000   \n",
       "mean     -0.002246    -0.004464    -0.022735    -0.017680    -0.003585   \n",
       "std       0.999763     0.997082     1.074310     0.923680     1.109777   \n",
       "min      -0.507900    -0.517100    -8.844321    -6.795061    -7.109165   \n",
       "25%      -0.507900    -0.517100    -0.170724    -0.461074    -0.396309   \n",
       "50%      -0.507900    -0.517100     0.004919    -0.122359     0.036526   \n",
       "75%      -0.507900    -0.517100     0.223773     0.390256     0.462439   \n",
       "max       4.060214     1.933862     9.403597     6.350112     8.662504   \n",
       "\n",
       "                5            6            7            8            9   \\\n",
       "count  1922.000000  1922.000000  1922.000000  1922.000000  1922.000000   \n",
       "mean     -0.003335    -0.004705    -0.008693    -0.009718    -0.009922   \n",
       "std       1.106825     1.098113     1.092914     1.091955     1.093523   \n",
       "min      -6.985810    -7.207462    -6.874485    -7.500630    -7.245063   \n",
       "25%      -0.401901    -0.478210    -0.497041    -0.523534    -0.501545   \n",
       "50%       0.032513     0.041032     0.077633     0.085518     0.074873   \n",
       "75%       0.446846     0.522221     0.536587     0.552471     0.561334   \n",
       "max       8.907937     7.604455     6.834938     7.752933     7.460513   \n",
       "\n",
       "                10           11           12           13           14  \\\n",
       "count  1922.000000  1922.000000  1922.000000  1922.000000  1922.000000   \n",
       "mean     -0.027411    -0.043538    -0.049059    -0.098692     0.000492   \n",
       "std       1.101376     1.127321     1.140101     1.113726     0.960511   \n",
       "min      -5.978411    -5.304133    -3.969464    -3.153637    -4.000111   \n",
       "25%      -0.569747    -0.603210    -0.569127    -0.455262    -0.556414   \n",
       "50%       0.171047     0.136405     0.051890     0.074842    -0.107183   \n",
       "75%       0.619494     0.705675     0.662916     0.559837     0.408253   \n",
       "max       4.839027     4.609906     3.799225     3.603865     8.560812   \n",
       "\n",
       "                15           16           17           18           19  \n",
       "count  1922.000000  1922.000000  1922.000000  1922.000000  1922.000000  \n",
       "mean     -0.008035     0.002379     0.010455     0.133389    -0.081174  \n",
       "std       0.880923     1.021252     1.048842     1.105402     0.952326  \n",
       "min      -2.340529    -4.223995    -4.355154    -0.990167    -1.405074  \n",
       "25%      -0.554569    -0.559025    -0.600749    -0.535700    -0.687520  \n",
       "50%      -0.124521    -0.076691    -0.051002    -0.167506    -0.298434  \n",
       "75%       0.387349     0.469229     0.528372     0.402930     0.234408  \n",
       "max       6.565746    11.206517     6.003443     8.404070     9.285350  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.006466</td>\n",
       "      <td>0.017832</td>\n",
       "      <td>0.079679</td>\n",
       "      <td>0.026721</td>\n",
       "      <td>0.000011</td>\n",
       "      <td>-0.000588</td>\n",
       "      <td>-0.001641</td>\n",
       "      <td>0.007858</td>\n",
       "      <td>0.009373</td>\n",
       "      <td>0.008198</td>\n",
       "      <td>0.050612</td>\n",
       "      <td>0.140175</td>\n",
       "      <td>0.244503</td>\n",
       "      <td>0.531242</td>\n",
       "      <td>0.015865</td>\n",
       "      <td>0.065817</td>\n",
       "      <td>-0.001668</td>\n",
       "      <td>-0.009547</td>\n",
       "      <td>-0.375440</td>\n",
       "      <td>0.115282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.996222</td>\n",
       "      <td>1.013402</td>\n",
       "      <td>0.638435</td>\n",
       "      <td>1.119913</td>\n",
       "      <td>0.623667</td>\n",
       "      <td>0.630154</td>\n",
       "      <td>0.676409</td>\n",
       "      <td>0.702497</td>\n",
       "      <td>0.706604</td>\n",
       "      <td>0.692251</td>\n",
       "      <td>0.611144</td>\n",
       "      <td>0.451226</td>\n",
       "      <td>0.355141</td>\n",
       "      <td>0.412784</td>\n",
       "      <td>1.077767</td>\n",
       "      <td>1.332116</td>\n",
       "      <td>0.933055</td>\n",
       "      <td>0.861707</td>\n",
       "      <td>0.511464</td>\n",
       "      <td>1.134053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>-2.786333</td>\n",
       "      <td>-5.364490</td>\n",
       "      <td>-2.797536</td>\n",
       "      <td>-3.058741</td>\n",
       "      <td>-4.076469</td>\n",
       "      <td>-4.441744</td>\n",
       "      <td>-4.454806</td>\n",
       "      <td>-4.337606</td>\n",
       "      <td>-2.123183</td>\n",
       "      <td>-1.647801</td>\n",
       "      <td>-1.172461</td>\n",
       "      <td>-0.718756</td>\n",
       "      <td>-2.874521</td>\n",
       "      <td>-2.609975</td>\n",
       "      <td>-4.629373</td>\n",
       "      <td>-2.865730</td>\n",
       "      <td>-1.033919</td>\n",
       "      <td>-1.159760</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>-0.277986</td>\n",
       "      <td>-0.574980</td>\n",
       "      <td>-0.299378</td>\n",
       "      <td>-0.312059</td>\n",
       "      <td>-0.332724</td>\n",
       "      <td>-0.320271</td>\n",
       "      <td>-0.318381</td>\n",
       "      <td>-0.311481</td>\n",
       "      <td>-0.269975</td>\n",
       "      <td>-0.059651</td>\n",
       "      <td>0.081024</td>\n",
       "      <td>0.305012</td>\n",
       "      <td>-0.586045</td>\n",
       "      <td>-0.624110</td>\n",
       "      <td>-0.487563</td>\n",
       "      <td>-0.581333</td>\n",
       "      <td>-0.729745</td>\n",
       "      <td>-0.610215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>0.086295</td>\n",
       "      <td>-0.042573</td>\n",
       "      <td>0.021022</td>\n",
       "      <td>0.012360</td>\n",
       "      <td>0.060728</td>\n",
       "      <td>0.078113</td>\n",
       "      <td>0.053703</td>\n",
       "      <td>0.080739</td>\n",
       "      <td>0.094618</td>\n",
       "      <td>0.177505</td>\n",
       "      <td>0.302295</td>\n",
       "      <td>0.520800</td>\n",
       "      <td>-0.077102</td>\n",
       "      <td>-0.087428</td>\n",
       "      <td>-0.087380</td>\n",
       "      <td>-0.155388</td>\n",
       "      <td>-0.502978</td>\n",
       "      <td>-0.143488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>0.485547</td>\n",
       "      <td>0.502434</td>\n",
       "      <td>0.350993</td>\n",
       "      <td>0.350933</td>\n",
       "      <td>0.367431</td>\n",
       "      <td>0.404540</td>\n",
       "      <td>0.410578</td>\n",
       "      <td>0.415818</td>\n",
       "      <td>0.417397</td>\n",
       "      <td>0.411836</td>\n",
       "      <td>0.448069</td>\n",
       "      <td>0.800260</td>\n",
       "      <td>0.536229</td>\n",
       "      <td>0.464823</td>\n",
       "      <td>0.422676</td>\n",
       "      <td>0.500613</td>\n",
       "      <td>-0.141730</td>\n",
       "      <td>0.463834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.918185</td>\n",
       "      <td>1.933862</td>\n",
       "      <td>2.203346</td>\n",
       "      <td>4.109946</td>\n",
       "      <td>3.021801</td>\n",
       "      <td>2.989399</td>\n",
       "      <td>3.686188</td>\n",
       "      <td>3.165570</td>\n",
       "      <td>2.389943</td>\n",
       "      <td>2.134465</td>\n",
       "      <td>1.909986</td>\n",
       "      <td>1.187349</td>\n",
       "      <td>1.015746</td>\n",
       "      <td>1.548766</td>\n",
       "      <td>6.181159</td>\n",
       "      <td>13.663933</td>\n",
       "      <td>3.897680</td>\n",
       "      <td>3.077722</td>\n",
       "      <td>3.350198</td>\n",
       "      <td>12.844461</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  504.000000  504.000000  504.000000  504.000000  504.000000  504.000000   \n",
       "mean     0.006466    0.017832    0.079679    0.026721    0.000011   -0.000588   \n",
       "std      0.996222    1.013402    0.638435    1.119913    0.623667    0.630154   \n",
       "min     -0.507900   -0.517100   -2.786333   -5.364490   -2.797536   -3.058741   \n",
       "25%     -0.507900   -0.517100   -0.277986   -0.574980   -0.299378   -0.312059   \n",
       "50%     -0.507900   -0.517100    0.086295   -0.042573    0.021022    0.012360   \n",
       "75%     -0.507900   -0.517100    0.485547    0.502434    0.350993    0.350933   \n",
       "max      2.918185    1.933862    2.203346    4.109946    3.021801    2.989399   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  504.000000  504.000000  504.000000  504.000000  504.000000  504.000000   \n",
       "mean    -0.001641    0.007858    0.009373    0.008198    0.050612    0.140175   \n",
       "std      0.676409    0.702497    0.706604    0.692251    0.611144    0.451226   \n",
       "min     -4.076469   -4.441744   -4.454806   -4.337606   -2.123183   -1.647801   \n",
       "25%     -0.332724   -0.320271   -0.318381   -0.311481   -0.269975   -0.059651   \n",
       "50%      0.060728    0.078113    0.053703    0.080739    0.094618    0.177505   \n",
       "75%      0.367431    0.404540    0.410578    0.415818    0.417397    0.411836   \n",
       "max      3.686188    3.165570    2.389943    2.134465    1.909986    1.187349   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  504.000000  504.000000  504.000000  504.000000  504.000000  504.000000   \n",
       "mean     0.244503    0.531242    0.015865    0.065817   -0.001668   -0.009547   \n",
       "std      0.355141    0.412784    1.077767    1.332116    0.933055    0.861707   \n",
       "min     -1.172461   -0.718756   -2.874521   -2.609975   -4.629373   -2.865730   \n",
       "25%      0.081024    0.305012   -0.586045   -0.624110   -0.487563   -0.581333   \n",
       "50%      0.302295    0.520800   -0.077102   -0.087428   -0.087380   -0.155388   \n",
       "75%      0.448069    0.800260    0.536229    0.464823    0.422676    0.500613   \n",
       "max      1.015746    1.548766    6.181159   13.663933    3.897680    3.077722   \n",
       "\n",
       "               18          19  \n",
       "count  504.000000  504.000000  \n",
       "mean    -0.375440    0.115282  \n",
       "std      0.511464    1.134053  \n",
       "min     -1.033919   -1.159760  \n",
       "25%     -0.729745   -0.610215  \n",
       "50%     -0.502978   -0.143488  \n",
       "75%     -0.141730    0.463834  \n",
       "max      3.350198   12.844461  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.004200</td>\n",
       "      <td>-0.001620</td>\n",
       "      <td>0.014045</td>\n",
       "      <td>0.081402</td>\n",
       "      <td>0.027319</td>\n",
       "      <td>0.026614</td>\n",
       "      <td>0.039167</td>\n",
       "      <td>0.050588</td>\n",
       "      <td>0.055376</td>\n",
       "      <td>0.059281</td>\n",
       "      <td>0.107843</td>\n",
       "      <td>0.051715</td>\n",
       "      <td>-0.114830</td>\n",
       "      <td>-0.309760</td>\n",
       "      <td>-0.035483</td>\n",
       "      <td>-0.070350</td>\n",
       "      <td>-0.014808</td>\n",
       "      <td>-0.060646</td>\n",
       "      <td>-0.266477</td>\n",
       "      <td>0.388549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1.015158</td>\n",
       "      <td>1.000838</td>\n",
       "      <td>1.001252</td>\n",
       "      <td>1.271094</td>\n",
       "      <td>0.680479</td>\n",
       "      <td>0.704905</td>\n",
       "      <td>0.722403</td>\n",
       "      <td>0.731715</td>\n",
       "      <td>0.734263</td>\n",
       "      <td>0.743440</td>\n",
       "      <td>0.783896</td>\n",
       "      <td>0.691611</td>\n",
       "      <td>0.562812</td>\n",
       "      <td>0.311627</td>\n",
       "      <td>1.130880</td>\n",
       "      <td>1.076941</td>\n",
       "      <td>0.970700</td>\n",
       "      <td>0.870239</td>\n",
       "      <td>0.550212</td>\n",
       "      <td>0.960356</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>-3.482311</td>\n",
       "      <td>-2.405700</td>\n",
       "      <td>-2.819101</td>\n",
       "      <td>-2.789370</td>\n",
       "      <td>-3.114412</td>\n",
       "      <td>-2.340853</td>\n",
       "      <td>-2.175100</td>\n",
       "      <td>-2.385657</td>\n",
       "      <td>-2.033764</td>\n",
       "      <td>-1.658450</td>\n",
       "      <td>-1.226221</td>\n",
       "      <td>-1.009647</td>\n",
       "      <td>-2.903366</td>\n",
       "      <td>-2.823122</td>\n",
       "      <td>-3.769979</td>\n",
       "      <td>-2.825518</td>\n",
       "      <td>-0.981178</td>\n",
       "      <td>-1.068757</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>-0.611052</td>\n",
       "      <td>-0.518717</td>\n",
       "      <td>-0.317475</td>\n",
       "      <td>-0.276751</td>\n",
       "      <td>-0.316887</td>\n",
       "      <td>-0.325984</td>\n",
       "      <td>-0.339072</td>\n",
       "      <td>-0.314260</td>\n",
       "      <td>-0.321826</td>\n",
       "      <td>-0.424870</td>\n",
       "      <td>-0.457919</td>\n",
       "      <td>-0.481317</td>\n",
       "      <td>-0.759143</td>\n",
       "      <td>-0.800195</td>\n",
       "      <td>-0.565420</td>\n",
       "      <td>-0.612964</td>\n",
       "      <td>-0.673336</td>\n",
       "      <td>-0.254535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>0.055638</td>\n",
       "      <td>-0.076000</td>\n",
       "      <td>0.026183</td>\n",
       "      <td>-0.003926</td>\n",
       "      <td>0.047736</td>\n",
       "      <td>0.059595</td>\n",
       "      <td>0.069553</td>\n",
       "      <td>0.111218</td>\n",
       "      <td>0.116825</td>\n",
       "      <td>0.200171</td>\n",
       "      <td>-0.185106</td>\n",
       "      <td>-0.357229</td>\n",
       "      <td>-0.153959</td>\n",
       "      <td>-0.208765</td>\n",
       "      <td>-0.094655</td>\n",
       "      <td>-0.148221</td>\n",
       "      <td>-0.408135</td>\n",
       "      <td>0.173110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>-0.507900</td>\n",
       "      <td>-0.517100</td>\n",
       "      <td>0.579683</td>\n",
       "      <td>0.506111</td>\n",
       "      <td>0.371059</td>\n",
       "      <td>0.383646</td>\n",
       "      <td>0.411371</td>\n",
       "      <td>0.448027</td>\n",
       "      <td>0.490706</td>\n",
       "      <td>0.502796</td>\n",
       "      <td>0.566689</td>\n",
       "      <td>0.561025</td>\n",
       "      <td>0.147872</td>\n",
       "      <td>-0.143241</td>\n",
       "      <td>0.559515</td>\n",
       "      <td>0.402551</td>\n",
       "      <td>0.415332</td>\n",
       "      <td>0.422095</td>\n",
       "      <td>0.007372</td>\n",
       "      <td>0.790819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>2.918185</td>\n",
       "      <td>1.933862</td>\n",
       "      <td>2.821884</td>\n",
       "      <td>11.199976</td>\n",
       "      <td>1.824283</td>\n",
       "      <td>1.888931</td>\n",
       "      <td>2.072088</td>\n",
       "      <td>2.599983</td>\n",
       "      <td>2.331683</td>\n",
       "      <td>2.153165</td>\n",
       "      <td>2.213261</td>\n",
       "      <td>1.442814</td>\n",
       "      <td>1.330106</td>\n",
       "      <td>0.605436</td>\n",
       "      <td>6.567692</td>\n",
       "      <td>4.102165</td>\n",
       "      <td>7.149330</td>\n",
       "      <td>2.920923</td>\n",
       "      <td>1.872788</td>\n",
       "      <td>4.920729</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               0           1           2           3           4           5   \\\n",
       "count  252.000000  252.000000  252.000000  252.000000  252.000000  252.000000   \n",
       "mean     0.004200   -0.001620    0.014045    0.081402    0.027319    0.026614   \n",
       "std      1.015158    1.000838    1.001252    1.271094    0.680479    0.704905   \n",
       "min     -0.507900   -0.517100   -3.482311   -2.405700   -2.819101   -2.789370   \n",
       "25%     -0.507900   -0.517100   -0.611052   -0.518717   -0.317475   -0.276751   \n",
       "50%     -0.507900   -0.517100    0.055638   -0.076000    0.026183   -0.003926   \n",
       "75%     -0.507900   -0.517100    0.579683    0.506111    0.371059    0.383646   \n",
       "max      2.918185    1.933862    2.821884   11.199976    1.824283    1.888931   \n",
       "\n",
       "               6           7           8           9           10          11  \\\n",
       "count  252.000000  252.000000  252.000000  252.000000  252.000000  252.000000   \n",
       "mean     0.039167    0.050588    0.055376    0.059281    0.107843    0.051715   \n",
       "std      0.722403    0.731715    0.734263    0.743440    0.783896    0.691611   \n",
       "min     -3.114412   -2.340853   -2.175100   -2.385657   -2.033764   -1.658450   \n",
       "25%     -0.316887   -0.325984   -0.339072   -0.314260   -0.321826   -0.424870   \n",
       "50%      0.047736    0.059595    0.069553    0.111218    0.116825    0.200171   \n",
       "75%      0.411371    0.448027    0.490706    0.502796    0.566689    0.561025   \n",
       "max      2.072088    2.599983    2.331683    2.153165    2.213261    1.442814   \n",
       "\n",
       "               12          13          14          15          16          17  \\\n",
       "count  252.000000  252.000000  252.000000  252.000000  252.000000  252.000000   \n",
       "mean    -0.114830   -0.309760   -0.035483   -0.070350   -0.014808   -0.060646   \n",
       "std      0.562812    0.311627    1.130880    1.076941    0.970700    0.870239   \n",
       "min     -1.226221   -1.009647   -2.903366   -2.823122   -3.769979   -2.825518   \n",
       "25%     -0.457919   -0.481317   -0.759143   -0.800195   -0.565420   -0.612964   \n",
       "50%     -0.185106   -0.357229   -0.153959   -0.208765   -0.094655   -0.148221   \n",
       "75%      0.147872   -0.143241    0.559515    0.402551    0.415332    0.422095   \n",
       "max      1.330106    0.605436    6.567692    4.102165    7.149330    2.920923   \n",
       "\n",
       "               18          19  \n",
       "count  252.000000  252.000000  \n",
       "mean    -0.266477    0.388549  \n",
       "std      0.550212    0.960356  \n",
       "min     -0.981178   -1.068757  \n",
       "25%     -0.673336   -0.254535  \n",
       "50%     -0.408135    0.173110  \n",
       "75%      0.007372    0.790819  \n",
       "max      1.872788    4.920729  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1922.000000</td>\n",
       "      <td>1922.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.443809</td>\n",
       "      <td>0.556191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.496962</td>\n",
       "      <td>0.496962</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 0            1\n",
       "count  1922.000000  1922.000000\n",
       "mean      0.443809     0.556191\n",
       "std       0.496962     0.496962\n",
       "min       0.000000     0.000000\n",
       "25%       0.000000     0.000000\n",
       "50%       0.000000     1.000000\n",
       "75%       1.000000     1.000000\n",
       "max       1.000000     1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>504.000000</td>\n",
       "      <td>504.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.478175</td>\n",
       "      <td>0.521825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500020</td>\n",
       "      <td>0.500020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1\n",
       "count  504.000000  504.000000\n",
       "mean     0.478175    0.521825\n",
       "std      0.500020    0.500020\n",
       "min      0.000000    0.000000\n",
       "25%      0.000000    0.000000\n",
       "50%      0.000000    1.000000\n",
       "75%      1.000000    1.000000\n",
       "max      1.000000    1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>252.000000</td>\n",
       "      <td>252.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.472222</td>\n",
       "      <td>0.527778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.500221</td>\n",
       "      <td>0.500221</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                0           1\n",
       "count  252.000000  252.000000\n",
       "mean     0.472222    0.527778\n",
       "std      0.500221    0.500221\n",
       "min      0.000000    0.000000\n",
       "25%      0.000000    0.000000\n",
       "50%      0.000000    1.000000\n",
       "75%      1.000000    1.000000\n",
       "max      1.000000    1.000000"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Split the data into train, validation and test sets:\n",
    "# Shuffling will not be used because test data should be drawn from the latest data points, to remove any risk of\n",
    "# the learning algorithms glimpsing the future\n",
    "\n",
    "train_dataset = X_scaled[756:]\n",
    "valid_dataset = X_scaled[252:756]\n",
    "test_dataset = X_scaled[:252]\n",
    "\n",
    "train_labels = y_scaled[756:]\n",
    "valid_labels = y_scaled[252:756]\n",
    "test_labels = y_scaled[:252]\n",
    "\n",
    "tf.cast(train_dataset, tf.float32)\n",
    "tf.cast(valid_dataset, tf.float32)\n",
    "tf.cast(test_dataset, tf.float32)\n",
    "\n",
    "tf.cast(train_labels, tf.float32)\n",
    "tf.cast(valid_labels, tf.float32)\n",
    "tf.cast(test_labels, tf.float32)\n",
    "\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)\n",
    "\n",
    "describe_train = pd.DataFrame(train_dataset)\n",
    "describe_valid = pd.DataFrame(valid_dataset)\n",
    "describe_test = pd.DataFrame(test_dataset)\n",
    "\n",
    "describe_train_labels = pd.DataFrame(train_labels)\n",
    "describe_valid_labels = pd.DataFrame(valid_labels)\n",
    "describe_test_labels = pd.DataFrame(test_labels)\n",
    "\n",
    "\n",
    "display(describe_train.describe())\n",
    "display(describe_valid.describe())\n",
    "display(describe_test.describe())\n",
    "\n",
    "display(describe_train_labels.describe())\n",
    "display(describe_valid_labels.describe())\n",
    "display(describe_test_labels.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "collapsed": true,
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "#Establish functions for calculating accuracy and F1 score\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])\n",
    "\n",
    "def F1_score(predictions, labels):\n",
    "  return f1_score(np.argmax(labels, 1), np.argmax(predictions, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Establish TensorFlow Graph\n",
    "\n",
    "batch_size = 1921\n",
    "num_labels = 2\n",
    "num_features = 20\n",
    "\n",
    "# Add second and third hidden layer, each with fewer nodes:\n",
    "\n",
    "num_nodes1 = 16\n",
    "num_nodes2 = 8\n",
    "num_nodes3 = 4\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  tf.set_random_seed(8)\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, num_features))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset,tf.float32)\n",
    "  tf_test_dataset = tf.constant(test_dataset,tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights_1 = tf.Variable(\n",
    "    tf.truncated_normal([num_features, num_nodes1],stddev=0.22))\n",
    "  biases_1 = tf.Variable(tf.zeros([num_nodes1]))\n",
    "  \n",
    "  weights_2 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes1, num_nodes2],stddev=0.32))\n",
    "  biases_2 = tf.Variable(tf.zeros([num_nodes2]))\n",
    "\n",
    "  weights_3 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes2, num_nodes3],stddev=0.45))\n",
    "  biases_3 = tf.Variable(tf.zeros([num_nodes3]))\n",
    "\n",
    "  weights_4 = tf.Variable(\n",
    "    tf.truncated_normal([num_nodes3, num_labels],stddev=0.71))\n",
    "  biases_4 = tf.Variable(tf.zeros([num_labels]))\n",
    "\n",
    "  # Training computation - Layer 1\n",
    "    \n",
    "  hidden1 = tf.nn.relu(tf.matmul(tf_train_dataset, weights_1) + biases_1)\n",
    "  \n",
    "  # Adding Dropout - Layer 1\n",
    "    \n",
    "  keep_prob1 = tf.placeholder(tf.float32)\n",
    "  hidden1_drop = tf.nn.dropout(hidden1,keep_prob1)\n",
    "    \n",
    "  # Training computation - Layer 2\n",
    "    \n",
    "  hidden2 = tf.nn.relu(tf.matmul(hidden1_drop, weights_2) + biases_2)\n",
    "  \n",
    "  # Adding Dropout - Layer 2\n",
    "    \n",
    "  keep_prob2 = tf.placeholder(tf.float32)\n",
    "  hidden2_drop = tf.nn.dropout(hidden2,keep_prob2)\n",
    "    \n",
    "  # Training computation - Layer 3\n",
    "    \n",
    "  hidden3 = tf.nn.relu(tf.matmul(hidden2_drop, weights_3) + biases_3)\n",
    "  \n",
    "  # Adding Dropout - Layer 3\n",
    "    \n",
    "  keep_prob3 = tf.placeholder(tf.float32)\n",
    "  hidden3_drop = tf.nn.dropout(hidden3,keep_prob3)\n",
    "    \n",
    "  logits = tf.matmul(hidden3_drop, weights_4) + biases_4\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits, tf_train_labels))\n",
    "    \n",
    "  # L2 regularization for the fully connected parameters.\n",
    "  regularizers = (tf.nn.l2_loss(weights_1) + tf.nn.l2_loss(biases_1) +\n",
    "                  tf.nn.l2_loss(weights_2) + tf.nn.l2_loss(biases_2) +\n",
    "                  tf.nn.l2_loss(weights_3) + tf.nn.l2_loss(biases_3) +\n",
    "                  tf.nn.l2_loss(weights_4) + tf.nn.l2_loss(biases_4))\n",
    "    \n",
    "  # Add the regularization term to the loss.\n",
    "  loss += 1e-6 * regularizers    \n",
    "  \n",
    "  # Optimizer - Add learning rate decay\n",
    "\n",
    "  global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "  learning_rate = tf.train.exponential_decay(\n",
    "        0.05, global_step, 100000, 0.98, staircase=True)\n",
    "    \n",
    "  optimizer = tf.train.GradientDescentOptimizer(\n",
    "    learning_rate).minimize(loss, global_step=global_step)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  \n",
    "  valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, weights_1)+\n",
    "                                                                       biases_1), weights_2)+ biases_2), weights_3)\n",
    "                             + biases_3), weights_4)+ biases_4)\n",
    "\n",
    "  test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, weights_1)+\n",
    "                                                                       biases_1), weights_2)+ biases_2), weights_3)\n",
    "                             + biases_3), weights_4)+ biases_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Training set restricted to the following number of samples:', 1921)\n",
      "('Training set', (1921, 20), (1921, 2))\n",
      "Initialized\n",
      "Minibatch loss at step 0: 0.699805\n",
      "Minibatch accuracy: 53.3%\n",
      "Validation accuracy: 52.4%\n",
      "Test accuracy: 52.78%\n",
      "F1 Score: 0.690909\n",
      "Minibatch loss at step 500: 0.665968\n",
      "Minibatch accuracy: 59.4%\n",
      "Validation accuracy: 60.3%\n",
      "Test accuracy: 59.92%\n",
      "F1 Score: 0.702065\n",
      "Minibatch loss at step 1000: 0.644152\n",
      "Minibatch accuracy: 63.9%\n",
      "Validation accuracy: 64.7%\n",
      "Test accuracy: 65.08%\n",
      "F1 Score: 0.702703\n",
      "Minibatch loss at step 1500: 0.623263\n",
      "Minibatch accuracy: 65.3%\n",
      "Validation accuracy: 64.7%\n",
      "Test accuracy: 63.10%\n",
      "F1 Score: 0.691030\n",
      "Minibatch loss at step 2000: 0.614810\n",
      "Minibatch accuracy: 66.5%\n",
      "Validation accuracy: 64.9%\n",
      "Test accuracy: 62.70%\n",
      "F1 Score: 0.688742\n",
      "Minibatch loss at step 2500: 0.602691\n",
      "Minibatch accuracy: 67.5%\n",
      "Validation accuracy: 64.9%\n",
      "Test accuracy: 63.10%\n",
      "F1 Score: 0.693069\n",
      "Minibatch loss at step 3000: 0.601838\n",
      "Minibatch accuracy: 67.2%\n",
      "Validation accuracy: 64.9%\n",
      "Test accuracy: 63.10%\n",
      "F1 Score: 0.688963\n",
      "Minibatch loss at step 3500: 0.594911\n",
      "Minibatch accuracy: 67.3%\n",
      "Validation accuracy: 65.1%\n",
      "Test accuracy: 62.70%\n",
      "F1 Score: 0.684564\n",
      "Minibatch loss at step 4000: 0.588233\n",
      "Minibatch accuracy: 67.9%\n",
      "Validation accuracy: 66.1%\n",
      "Test accuracy: 64.29%\n",
      "F1 Score: 0.695946\n",
      "Minibatch loss at step 4500: 0.576514\n",
      "Minibatch accuracy: 68.6%\n",
      "Validation accuracy: 66.1%\n",
      "Test accuracy: 65.87%\n",
      "F1 Score: 0.709459\n",
      "Minibatch loss at step 5000: 0.571735\n",
      "Minibatch accuracy: 69.3%\n",
      "Validation accuracy: 66.1%\n",
      "Test accuracy: 66.27%\n",
      "F1 Score: 0.711864\n",
      "Minibatch loss at step 5500: 0.562568\n",
      "Minibatch accuracy: 70.4%\n",
      "Validation accuracy: 64.9%\n",
      "Test accuracy: 67.46%\n",
      "F1 Score: 0.719178\n",
      "Minibatch loss at step 6000: 0.560957\n",
      "Minibatch accuracy: 70.4%\n",
      "Validation accuracy: 65.3%\n",
      "Test accuracy: 67.06%\n",
      "F1 Score: 0.716724\n",
      "Minibatch loss at step 6500: 0.550395\n",
      "Minibatch accuracy: 71.1%\n",
      "Validation accuracy: 65.7%\n",
      "Test accuracy: 67.46%\n",
      "F1 Score: 0.719178\n",
      "Minibatch loss at step 7000: 0.544491\n",
      "Minibatch accuracy: 71.4%\n",
      "Validation accuracy: 65.1%\n",
      "Test accuracy: 68.25%\n",
      "F1 Score: 0.726027\n",
      "\n",
      "\n",
      "FINAL Test accuracy: 68.25%\n",
      "FINAL F1 Score: 0.726027\n"
     ]
    }
   ],
   "source": [
    "num_steps = 7001\n",
    "\n",
    "# Set train_batches to 8 to use the full training set\n",
    "\n",
    "train_batches = 1\n",
    "train_subset = train_batches * batch_size\n",
    "\n",
    "print('Training set restricted to the following number of samples:',train_subset)\n",
    "\n",
    "train_smallset = train_dataset[0:train_subset,:]\n",
    "train_smalllabel = train_labels[0:train_subset]\n",
    "\n",
    "print('Training set', train_smallset.shape, train_smalllabel.shape)\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "\n",
    "  print(\"Initialized\")\n",
    "    \n",
    "  step_counter = []\n",
    "  step_train_accuracy = []\n",
    "  step_valid_accuracy = []\n",
    "  step_test_accuracy = []\n",
    "    \n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "#    offset = (step * batch_size) % (train_smalllabel.shape[0] - batch_size)\n",
    "    offset = 0\n",
    "\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_smallset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_smalllabel[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels,keep_prob1: 0.95,\n",
    "                 keep_prob2:0.95,keep_prob3:0.95}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    \n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(valid_prediction.eval(), valid_labels))\n",
    "      print(\"Test accuracy: %.2f%%\" % accuracy(test_prediction.eval(), test_labels))   \n",
    "      print (\"F1 Score: %f\" % F1_score(test_prediction.eval(), test_labels))\n",
    "      \n",
    "      step_counter.append(step)\n",
    "      step_train_accuracy.append(accuracy(predictions, batch_labels))\n",
    "      step_valid_accuracy.append(accuracy(valid_prediction.eval(), valid_labels))\n",
    "      step_test_accuracy.append(accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "  print '\\n'\n",
    "  print(\"FINAL Test accuracy: %.2f%%\" % accuracy(test_prediction.eval(), test_labels))   \n",
    "  print (\"FINAL F1 Score: %f\" % F1_score(test_prediction.eval(), test_labels))\n",
    "    \n",
    "# Output predictions for further analysis\n",
    "  np.savetxt(\"eval-NN.csv\", test_prediction.eval(), delimiter=\",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11baa2c50>"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYMAAAEPCAYAAACgFqixAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nFyfl+keSAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11baa2690>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(step_train_accuracy, label = 'train')\n",
    "plt.plot(step_valid_accuracy, label = 'validate')\n",
    "plt.plot(step_test_accuracy, label = 'test')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.xlabel('Number of 500 Step Training Batches')\n",
    "plt.legend(loc=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Trading Simulation\n",
    "\n",
    "The model's performance will now be compared to two benchmark trading strategies, over the 252 day test set (10/01/2015 - 09/29/2016):\n",
    "\n",
    "1. Buy and hold - the index is purchased at the opening price on the first day of the test period and then sold at the closing price of the last day of the test period.\n",
    "\n",
    "2. Buy only - since always buying has a slight (52-54%) accuracy advantage over selling, the index is bought each day at the opening price and then sold at the closing price. This strategy is repeated each day, in contrast to the 'buy and hold' approach, which involves a single buy and a single sell event.\n",
    "\n",
    "The model itself is evaluated as follows: if the model predicts the price will close higher, then the index is bought at the open and sold at the close. If the model predicts the price will close lower, then the index is sold at the open and bought at the close."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cumulative Buy and Hold Strategy Profit ($): 231.48\n",
      "Cumulative Buy Only Strategy Profit ($): 234.98\n",
      "Cumulative Machine Learning Strategy Profit ($): 1793.4\n"
     ]
    }
   ],
   "source": [
    "# Identify and format data needed for trading profit calculations\n",
    "\n",
    "validated_data_limited = pd.DataFrame(validated_data.loc[:,'Date':'SP_Close'])\n",
    "validated_data_limited = validated_data_limited[0:252]\n",
    "\n",
    "# Read in the NN Model predictions:\n",
    "ML_trading_data = pd.read_csv(\"eval-NN.csv\",header=None)\n",
    "\n",
    "# If column 1 > column 0, then the label (prediction) is '1' (i.e. S&P500 is predicted to increase that day)\n",
    "ML_trading_data['Predicted_Up'] = ML_trading_data[0] + ML_trading_data[1]\n",
    "\n",
    "ML_trading_data.loc[ML_trading_data[1]>0.5,'Predicted_Up'] = 1\n",
    "ML_trading_data.loc[ML_trading_data[1]<=0.5,'Predicted_Up'] = 0\n",
    "\n",
    "# Append the model predictions to the market data:\n",
    "\n",
    "validated_data_limited['Predicted_Up'] = ML_trading_data['Predicted_Up']\n",
    "\n",
    "# Reverse the dataframe order, so dates go from oldest to newest:\n",
    "validated_data_limited = validated_data_limited.iloc[::-1]\n",
    "\n",
    "# Buy and hold benchmark calculation:\n",
    "\n",
    "validated_data_limited['Cum_Buy_Hold_Profit'] = validated_data_limited['SP_Close'] - (\n",
    "    validated_data_limited['SP_Open'][251])\n",
    "\n",
    "# Buy only benchmark calculation:\n",
    "\n",
    "validated_data_limited['Buy_Only_Profit'] = validated_data_limited['SP_Close'] - validated_data_limited['SP_Open']\n",
    "validated_data_limited['Cum_Buy_Only_Profit'] = np.cumsum(validated_data_limited['Buy_Only_Profit'])\n",
    "\n",
    "# NN model results:\n",
    "# Model profit is equal to the buy only profit, adjusted with a negative sign if the event is a sell instead:\n",
    "\n",
    "validated_data_limited['Model_Profit'] = validated_data_limited['Buy_Only_Profit'] * (\n",
    "    2*validated_data_limited['Predicted_Up'] -1)\n",
    "\n",
    "validated_data_limited['Cum_Model_Profit'] = np.cumsum(validated_data_limited['Model_Profit'])\n",
    "\n",
    "# Output the final cumulative profit for the ML strategy and the benchmarks:\n",
    "\n",
    "print \"Cumulative Buy and Hold Strategy Profit ($):\", validated_data_limited['Cum_Buy_Hold_Profit'][0]\n",
    "print \"Cumulative Buy Only Strategy Profit ($):\", validated_data_limited['Cum_Buy_Only_Profit'][0]\n",
    "print \"Cumulative Machine Learning Strategy Profit ($):\", validated_data_limited['Cum_Model_Profit'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x11e2a0e90>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYwAAAEKCAYAAAAB0GKPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nMzOrxQnDzMxq+R8GfTRqpbJM5AAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x11b1e3610>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot the cumulative performance of the ML trading strategy compared to the benchmarks:\n",
    "\n",
    "trading_data = validated_data_limited\n",
    "\n",
    "plt.axis([252,0,-100,1800])\n",
    "\n",
    "plt.plot(trading_data['Cum_Buy_Hold_Profit'], label = 'buy & hold')\n",
    "plt.plot(trading_data['Cum_Buy_Only_Profit'], label = 'buy only')\n",
    "plt.plot(trading_data['Cum_Model_Profit'], label = 'ML trading')\n",
    "\n",
    "plt.ylabel('Cumulative Profit ($)')\n",
    "plt.xlabel('Look back Days')\n",
    "plt.legend(loc=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buy Only Daily Profit Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    252.000000\n",
       "mean       0.932460\n",
       "std       16.449823\n",
       "min      -66.400000\n",
       "25%       -7.320000\n",
       "50%        0.810000\n",
       "75%        9.092500\n",
       "max       46.240000\n",
       "Name: Buy_Only_Profit, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "ML Trading Daily Profit Summary Statistics:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "count    252.000000\n",
       "mean       7.116667\n",
       "std       14.853313\n",
       "min      -50.060000\n",
       "25%       -1.880000\n",
       "50%        5.175000\n",
       "75%       17.052500\n",
       "max       66.400000\n",
       "Name: Model_Profit, dtype: float64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print \"Buy Only Daily Profit Summary Statistics:\"\n",
    "display(validated_data_limited['Buy_Only_Profit'].describe())\n",
    "print '\\n'\n",
    "print \"ML Trading Daily Profit Summary Statistics:\"\n",
    "display(validated_data_limited['Model_Profit'].describe())"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
